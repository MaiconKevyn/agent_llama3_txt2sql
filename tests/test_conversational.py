#!/usr/bin/env python3
"""
Test script for conversational LLM implementation.
"""

import sys
import os

# Add parent directory and src to path
parent_dir = os.path.dirname(os.path.dirname(__file__))
sys.path.append(parent_dir)
sys.path.append(os.path.join(parent_dir, 'src'))

from src.application.services.conversational_llm_service import ConversationalLLMService
from src.application.services.conversational_response_service import ConversationalResponseService
from src.application.services.sus_prompt_template_service import SUSPromptTemplateService, PromptType

def test_conversational_service():
    """Test the conversational service components."""
    
    print("ğŸ§ª Testing Conversational LLM Implementation")
    print("=" * 50)
    
    # Test 1: ConversationalLLMService initialization
    print("\n1ï¸âƒ£ Testing ConversationalLLMService...")
    try:
        conv_llm = ConversationalLLMService()
        print(f"âœ… ConversationalLLMService initialized")
        print(f"ğŸ“‹ Model: {conv_llm.config.model_name}")
        print(f"ğŸŒ¡ï¸ Temperature: {conv_llm.config.temperature}")
        print(f"ğŸ”— Available: {conv_llm.is_available()}")
    except Exception as e:
        print(f"âŒ ConversationalLLMService failed: {e}")
        return False
    
    # Test 2: SUSPromptTemplateService
    print("\n2ï¸âƒ£ Testing SUSPromptTemplateService...")
    try:
        prompt_service = SUSPromptTemplateService()
        templates = prompt_service.get_available_templates()
        print(f"âœ… SUSPromptTemplateService initialized")
        print(f"ğŸ“ Available templates: {len(templates)}")
        
        # Test prompt type detection
        test_query = "Quantos pacientes existem por estado?"
        prompt_type = prompt_service.determine_prompt_type(test_query)
        print(f"ğŸ” Query '{test_query}' detected as: {prompt_type.value}")
        
    except Exception as e:
        print(f"âŒ SUSPromptTemplateService failed: {e}")
        return False
    
    # Test 3: ConversationalResponseService
    print("\n3ï¸âƒ£ Testing ConversationalResponseService...")
    try:
        conv_response = ConversationalResponseService()
        print(f"âœ… ConversationalResponseService initialized")
        print(f"ğŸ¤– LLM Available: {conv_response.is_conversational_llm_available()}")
        
        # Test status
        status = conv_response.get_service_status()
        print(f"ğŸ“Š Service Status:")
        for key, value in status.items():
            print(f"   â€¢ {key}: {value}")
            
    except Exception as e:
        print(f"âŒ ConversationalResponseService failed: {e}")
        return False
    
    # Test 4: End-to-end conversational response (if LLM is available)
    print("\n4ï¸âƒ£ Testing End-to-End Conversational Response...")
    try:
        if conv_response.is_conversational_llm_available():
            print("ğŸš€ Testing conversational response generation...")
            
            # Simulate query results
            test_query = "Quantos pacientes temos no total?"
            test_sql = "SELECT COUNT(*) as total_pacientes FROM pacientes"
            test_results = [{"total_pacientes": 1234}]
            
            response = conv_response.generate_response(
                user_query=test_query,
                sql_query=test_sql,
                sql_results=test_results,
                session_id="test_session"
            )
            
            print(f"âœ… Conversational response generated!")
            print(f"ğŸ“ Response type: {response.response_type.value}")
            print(f"â±ï¸ Processing time: {response.processing_time:.2f}s")
            print(f"ğŸ¯ Confidence: {response.confidence_score:.2f}")
            print(f"ğŸ’¡ Suggestions: {len(response.suggestions)}")
            print(f"ğŸ“„ Response preview: {response.message[:200]}...")
            
        else:
            print("âš ï¸ Conversational LLM not available - skipping end-to-end test")
            print("ğŸ’¡ Make sure Ollama is running with llama3.1:8b-instruct model")
            
    except Exception as e:
        print(f"âŒ End-to-end test failed: {e}")
        return False
    
    print("\n" + "=" * 50)
    print("ğŸ‰ All conversational tests completed successfully!")
    return True

if __name__ == "__main__":
    success = test_conversational_service()
    sys.exit(0 if success else 1)